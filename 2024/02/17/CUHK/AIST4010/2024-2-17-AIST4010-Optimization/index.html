<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Optimization | myBlog</title><meta name="keywords" content="Deep Learning"><meta name="author" content="Donald Lam,manholam8@gmail.com"><meta name="copyright" content="Donald Lam"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="Gradient Descent Epoch: the time that we iterative through all the training data. Batch Size: the number of data points that we feed to the algorithm every iteration. An Iteration: the time that the w">
<meta property="og:type" content="article">
<meta property="og:title" content="Optimization">
<meta property="og:url" content="https://github.com/DonaldLamNL/myBlog/2024/02/17/CUHK/AIST4010/2024-2-17-AIST4010-Optimization/index.html">
<meta property="og:site_name" content="myBlog">
<meta property="og:description" content="Gradient Descent Epoch: the time that we iterative through all the training data. Batch Size: the number of data points that we feed to the algorithm every iteration. An Iteration: the time that the w">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://p.ipic.vip/llqylk.jpg">
<meta property="article:published_time" content="2024-02-16T16:00:00.000Z">
<meta property="article:modified_time" content="2024-02-16T16:00:00.000Z">
<meta property="article:author" content="Donald Lam">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://p.ipic.vip/llqylk.jpg"><link rel="shortcut icon" href="/myBlog/img/favicon.png"><link rel="canonical" href="https://github.com/DonaldLamNL/myBlog/2024/02/17/CUHK/AIST4010/2024-2-17-AIST4010-Optimization/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/myBlog/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/myBlog/',
  algolia: undefined,
  localSearch: {"path":"/myBlog/search.xml","preload":true,"languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Optimization',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-02-17 00:00:00'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://p.ipic.vip/5dc8z2.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/myBlog/archives/"><div class="headline">Articles</div><div class="length-num">71</div></a><a href="/myBlog/tags/"><div class="headline">Tags</div><div class="length-num">14</div></a><a href="/myBlog/categories/"><div class="headline">Categories</div><div class="length-num">14</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/myBlog/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/myBlog/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/myBlog/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/myBlog/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://p.ipic.vip/llqylk.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/myBlog/">myBlog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/myBlog/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/myBlog/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/myBlog/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/myBlog/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Optimization</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-02-16T16:00:00.000Z" title="Created 2024-02-17 00:00:00">2024-02-17</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-02-16T16:00:00.000Z" title="Updated 2024-02-17 00:00:00">2024-02-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/myBlog/categories/AIST4010/">AIST4010</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Optimization"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><ul>
<li>Epoch: the time that we iterative through all the training data.</li>
<li>Batch Size: the number of data points that we feed to the algorithm every iteration.</li>
<li>An Iteration: the time that the weights update.</li>
</ul>
<p>Given the steps of gradient descent algorithm,</p>
<ul>
<li><strong>Algorithm:</strong><ol>
<li>Initialize the weights.</li>
<li>For all the training data: forward pass and calculate the output(s) $Y$.</li>
<li>Update the weights (gradient descent):<script type="math/tex; mode=display">\Delta w_{ij}^{(k)} = \alpha\frac{\partial Div(Y, d)}{\partial w_{ij}^{(k)}}</script><script type="math/tex; mode=display">w_{ij}^{(k)} = w_{ij}^{(k)} - \Delta w_{ij}^{(k)}</script></li>
<li>Repeat from step 1.</li>
</ol>
</li>
</ul>
<h3 id="Batch-Gradient-Descent"><a href="#Batch-Gradient-Descent" class="headerlink" title="Batch Gradient Descent"></a>Batch Gradient Descent</h3><p>We can use GPU to accelerate the computations, but we cannot fit all the data into the GPU memory as the GPU is small. Therefore, we consider feeding a batch of data each time.</p>
<ul>
<li><p><strong>Min-Batch Gradient Descent</strong><br>This algorithm uses a batch of samples to update the weights every iteration.</p>
</li>
<li><p><strong>Stochastic Gradient Descent</strong><br>This algorithm uses 1 sample to update the weights every iteration.</p>
</li>
</ul>
<p>However, if the model is too large while the batch is too small, the training can be very unstable.</p>
<font color="3A75EA">Any solutions can make the convergence more stable?</font>



<h3 id="Accumulate-Gradient"><a href="#Accumulate-Gradient" class="headerlink" title="Accumulate Gradient"></a>Accumulate Gradient</h3><p>We can accumulate the gradient for like 10 iterations to update the weights.</p>
<ul>
<li><strong>Algorithm:</strong><br><code>For t=1:10</code><script type="math/tex; mode=display">\Delta w_{ij}^{(k)} \mathrel{+}= \alpha\frac{\partial Div(Y_{B_t}, d_{B_t})}{\partial w_{ij}^{(k)}}</script><script type="math/tex; mode=display">w_{ij}^{(k)} = w_{ij}^{(k)} - \Delta w_{ij}^{(k)}</script></li>
</ul>
<font color="3A75EA">Any solutions can make the convergence faster and better?</font>



<h2 id="Momentum-Optimization"><a href="#Momentum-Optimization" class="headerlink" title="Momentum Optimization"></a>Momentum Optimization</h2><h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><ul>
<li><strong>Learning Rate:</strong> It is hard to set the learning rate, if too small will take more steps to converge, while too large may lead to <font color="F54747">oscillation</font> which slows down the convergence. Momentum extension focuses on improving the selection of the learning rate.</li>
<li><p><strong>Idea:</strong> By remembering the update of the previous step, the learning rate $\alpha$ is affected by the sign of both previous and current gradients. If they have the same gradient sign, the update step is longer in directions; otherwise, the update step is shorter in directions.</p>
</li>
<li><p><strong>Algorithm:</strong><br>  <code>For each iteration</code></p>
<ol>
<li>Compute the gradient at the current location</li>
<li>Add the scaled previous step<script type="math/tex; mode=display">w^{\,t} = w^{\,t-1} - \alpha\frac{\partial Loss}{\partial w^{\,t-1}} + \beta \Delta w^{\,t-1}</script></li>
</ol>
<ul>
<li>$\Delta w^{\,t-1} = w^{\,t-1} - w^{\,t-2}$</li>
<li>Typical $\alpha = 0.001$, $\beta = 0.9$</li>
</ul>
</li>
<li><p><strong><font color="F54747">Why momentum can conduct a smoother and faster convergence?</font></strong><br>It actually maintains a moving average of the gradient, which can amplify the learning rate in the correct direction, which smoothes the updates and enlarges the learning rate if the gradient signs are the same.</p>
</li>
</ul>
<h3 id="Nestorov’s-Momentum"><a href="#Nestorov’s-Momentum" class="headerlink" title="Nestorov’s Momentum"></a>Nestorov’s Momentum</h3><p>The difference between Momentum and Nesterov’s Momentum is in the gradient computation phase. The Nesterov’s Momentum will take an extension of the previous step first, then compute the gradient at the location (after extension).</p>
<ul>
<li><p><strong>Algorithm:</strong><br>  <code>For each iteration</code></p>
<ol>
<li>Extend the previous step</li>
<li>Compute the gradient at the current location</li>
<li>Sum them up<script type="math/tex; mode=display">\begin{align*}
w^{\,t} = w^{\,t-1} - \alpha\frac{\partial Loss}{\partial (w^{\,t-1} + \beta \Delta w^{\,t-1})} + \beta \Delta w^{\,t-1}
\end{align*}</script><img src="https://p.ipic.vip/bfkpjh.png" width="500px" alt="Momentum Method (Left), Nestorov's Momentum (Right)" /></li>
</ol>
</li>
<li><p><strong><font color="3A75EA">Any further improvement?</font></strong></p>
<ul>
<li><strong>Observation:</strong> The steps in the oscillatory direction still show a large total movement.</li>
<li><strong>Improvement:</strong> We want to reduce the step size in directions with high motion and increase it in directions with slow motion.</li>
<li><strong>Methodology:</strong> We can <font color="F54747">scale</font> the updates in every component in <font color="F54747">inverse proportion</font> to the total movement of that component in recent past.</li>
</ul>
</li>
</ul>
<p><img src="https://p.ipic.vip/u9axaq.png" width="500px" alt="" /></p>
<h2 id="Scaling"><a href="#Scaling" class="headerlink" title="Scaling"></a>Scaling</h2><h3 id="Inverse-mean-square-derivative"><a href="#Inverse-mean-square-derivative" class="headerlink" title="Inverse mean square derivative"></a>Inverse mean square derivative</h3><p>Recall the update of weights: $w^{\,t} = w^{\,t-1} - \alpha \frac{\partial Loss}{\partial w^{\,t-1}}$.<br>Let $\partial_{w^{\,t-1}}L = \frac{\partial Loss}{\partial w^{\,t-1}}$ be the derivative of loss with respect to the parameter $w^{\,t-1}$ (i.e., the gradient).</p>
<ul>
<li>$(\partial_{w^{\,t-1}}L)^2$ denotes the squared derivative, <font color="F54747">not the second derivative</font></li>
<li>$E[(\partial_{w}L)^2]$ denotes the expectation of the square derivative.</li>
<li>$E[(\partial_{w}L)^2]_{t-1}$ denotes the expectation of the square derivative <font color="F54747">until step $t-1$</font></li>
</ul>
<p>Now the step is scaled by the inverse mean square derivative,</p>
<script type="math/tex; mode=display">\begin{align*}
    w^{\,t} = w^{\,t-1} - \frac{\alpha}{E[(\partial_{w}L)^2]_{t-1}} \cdot \partial w^{\,t-1}L
\end{align*}</script><p>Since the number of iterations may be very large, it is expensive to store all previous squared derivative. Instead of accurately compute the expectation of the square derivative until the step, we apply a <font color="F54747">running average</font> to estimate it,</p>
<script type="math/tex; mode=display">\begin{align*}
    E[(\partial_{w}L)^2]_{t} = \begin{cases}
        \,\, \gamma\, E[(\partial_{w}L)^2]_{t-1} + (1-\gamma)(\partial_{w^{\,t}}L)^2 & \text{if } t \neq 0 \\
        \,\, 0 & \text{if } t = 0
    \end{cases}
\end{align*}</script><p>To prevent the divide by 0 problem, we add $\epsilon$ to $E[(\partial_{w}L)^2]_{t-1}$,</p>
<script type="math/tex; mode=display">\begin{align*}
    w^{\,t} = w^{\,t-1} - \frac{\alpha}{E[(\partial_{w}L)^2]_{t-1} + \epsilon} \cdot \partial w^{\,t-1}L
\end{align*}</script><h3 id="Root-Mean-Squared-Propagation"><a href="#Root-Mean-Squared-Propagation" class="headerlink" title="Root Mean Squared Propagation"></a>Root Mean Squared Propagation</h3><script type="math/tex; mode=display">\begin{align*}
    w^{\,t} = w^{\,t-1} - \frac{\alpha}{\sqrt{E[(\partial_{w}L)^2]_{t-1} + \epsilon}} \cdot \partial w^{\,t-1}L
\end{align*}</script><ul>
<li>$E[(\partial_{w}L)^2]_{t-1} = \gamma\, E[(\partial_{w}L)^2]_{t-2} + (1-\gamma)(\partial_{w^{\,t-1}}L)^2$</li>
<li>$E[(\partial_{w}L)^2]_0 = 0$</li>
<li>Typical $\gamma = 0.9$, $\alpha = 0.001$, $\epsilon = 10^{-7}$</li>
</ul>
<h2 id="Adaptive-moment-estimation"><a href="#Adaptive-moment-estimation" class="headerlink" title="Adaptive moment estimation"></a>Adaptive moment estimation</h2><h3 id="RMS-Prop-Momentum"><a href="#RMS-Prop-Momentum" class="headerlink" title="RMS Prop + Momentum"></a>RMS Prop + Momentum</h3><p>Recall the update of momentum method: $w^{\,t} = w^{\,t-1} - \alpha \frac{\partial Loss}{\partial w^{\,t-1}} + \beta \Delta w^{\,t-1}$, where $\Delta w^{\,t-1} = w^{\,t-1} - w^{\,t-2}$.</p>
<p>We can determine $w^{\,t}$ as following,</p>
<script type="math/tex; mode=display">\begin{align*}
    w^{\,t} &= w^{\,t-1} - \alpha \frac{\partial Loss}{\partial w^{\,t-1}} + \beta \Delta w^{\,t-1} \\
    &= w^{\,t-1} - \alpha \frac{\partial Loss}{\partial w^{\,t-1}} + \beta (w^{\,t-1} - w^{\,t-2}) \\
    &= w^{\,t-1} - \alpha \big( \frac{\partial Loss}{\partial w^{\,t-1}} - \frac{\beta}{\alpha} (w^{\,t-1} - w^{\,t-2}) \big).
\end{align*}</script><p>We find the recursive structure,</p>
<script type="math/tex; mode=display">\begin{align*}
    w^{\,t} -  w^{\,t-1} &= - \alpha \frac{\partial Loss}{\partial w^{\,t-1}} + \beta (w^{\,t-1} - w^{\,t-2}) \\
    w^{\,t-1} -  w^{\,t-2} &= - \alpha \frac{\partial Loss}{\partial w^{\,t-2}} + \beta (w^{\,t-2} - w^{\,t-3}) \\
    & \quad\cdots\\
    w^{\,2} -  w^{\,1} &= - \alpha \frac{\partial Loss}{\partial w^{\,1}} + \beta (w^{\,1} - w^{\,0}).
\end{align*}</script><p>We can determine the $w^{\,t}$ by apply the recursive structure,</p>
<script type="math/tex; mode=display">\begin{align*}
    w^{\,t} &= w^{\,t-1} - \alpha \Big( \frac{\partial Loss}{\partial w^{\,t-1}} - \frac{\beta}{\alpha} \big(- \alpha \frac{\partial Loss}{\partial w^{\,t-2}} + \beta (w^{\,t-2} - w^{\,t-3}) \big) \Big) \\
    &= w^{\,t-1} - \alpha \big( \frac{\partial Loss}{\partial w^{\,t-1}} + \beta \frac{\partial Loss}{\partial w^{\,t-2}} - \frac{\beta^2}{\alpha} (w^{\,t-2} -  w^{\,t-3}) \big) \\
    & \quad\cdots\\
    &= w^{\,t-1} - \alpha \big( \frac{\partial Loss}{\partial w^{\,t-1}} + \beta \frac{\partial Loss}{\partial w^{\,t-2}} + \cdots + \beta^{\,t-1} \frac{\partial Loss}{\partial w^{\,0}} + 0\big).
\end{align*}</script><p>Note that the terms <font color="3A75EA">$\frac{\partial Loss}{\partial w^{\,t-1}} + \beta \frac{\partial Loss}{\partial w^{\,t-2}} + \cdots + \beta^{\,t-1} \frac{\partial Loss}{\partial w^{\,0}} + 0$</font> related to ALL previous derivatives, which is the <font color="F54747">running average</font>. Therefore, we can unify the expression,</p>
<script type="math/tex; mode=display">\begin{align*}
    w^{\,t} = w^{\,t-1} - \frac{\alpha}{\sqrt{E[(\partial_{w}L)^2]_{t-1} + \epsilon}} \cdot E[\partial_wL]_{t-1}
\end{align*}</script><ul>
<li>$E[(\partial_{w}L)^2]_{t-1} = \gamma\, E[(\partial_{w}L)^2]_{t-2} + (1-\gamma)(\partial_{w^{\,t-1}}L)^2$.</li>
<li><font color="3A75EA">$E[\partial_wL]_{t-1} = \delta E[\partial_wL]_{t-2} + (1-\delta)\partial_{w^{\,t-1}}L$</font>, which is similar to the RMS Prop.</li>
<li>The boundary cases: $E[(\partial_{w}L)^2]_0 = 0$ and $E[\partial_wL]_0 = 0$.</li>
<li>Typical $\gamma = 0.9$, $\alpha = 0.001$, $\epsilon = 10^{-7}$</li>
</ul>
<h3 id="Expected-running-average"><a href="#Expected-running-average" class="headerlink" title="Expected running average"></a>Expected running average</h3><p>Let $m_t$ be the running average until step t.<br>Let $g_t$ be the value of each observation at step t.</p>
<script type="math/tex; mode=display">\begin{align*}
    m_t &= \delta m_{t-1} + (1-\delta) g_t \\
    &= \delta^2 m_{t-2} + \delta(1-\delta) g_{t-1} + (1-\delta) g_t \\
    &\cdots\\
    &= (1-\delta)\delta^{\,t-1}g_1 + \cdots + (1-\delta)\delta g_{t-1} + (1-\delta) g_{t}\\
    &= (1-\delta)\sum_{i=0}^t \delta^{\,t-i}g_i.
\end{align*}</script><p>Take the expectation on  both sides,</p>
<script type="math/tex; mode=display">\begin{align*}
    E[m_t] &= E[(1-\delta)\sum_{i=0}^t \delta^{\,t-i}g_i]
\end{align*}</script><p>We believe that the observation of $g_i$ is around $g_t$, so we let $g_i = g_t + \varsigma$,</p>
<script type="math/tex; mode=display">\begin{align*}
    E[m_t] &= E[(1-\delta)\sum_{i=0}^t \delta^{\,t-i}(g_t + \varsigma)] \\
    E[m_t] &= E[g_t] \cdot (1-\delta)\sum_{i=0}^t \delta^{\,t-i} + \varsigma' \\
    E[m_t] &= E[g_t] \cdot (1-\delta^t) + \varsigma'
\end{align*}</script><p>If the running average is good, <font color="F54747">$E[m_t] = E[g_t]$</font>.</p>
<h3 id="Running-average-correction"><a href="#Running-average-correction" class="headerlink" title="Running average correction"></a>Running average correction</h3><p>Since running average is not an unbiased estimator, we need to add a correction term.</p>
<script type="math/tex; mode=display">\begin{align*}
    \hat{E}[(\partial_wL)^2]_{t-1} = \frac{E[(\partial_wL)^2]_{t-1}}{1-\gamma^{\,\,t-1}} \text{, } \hat{E}[\partial_wL]_{t-1}\frac{E[\partial_wL]_{t-1}}{1-\delta^{\,t-1}}.
\end{align*}</script><p>If $t$ is small, $1-\gamma^{\,\,t-1}$ is small, which means not correlate.</p>
<h3 id="Adam-formula"><a href="#Adam-formula" class="headerlink" title="Adam formula"></a>Adam formula</h3><script type="math/tex; mode=display">\begin{align*}
    w^{\,t} = w^{\,t-1} - \frac{\alpha}{\sqrt{\hat{E}[(\partial_{w}L)^2]_{t-1} + \epsilon}} \cdot \hat{E}[\partial_wL]_{t-1}
\end{align*}</script><ul>
<li>$\hat{E}[(\partial_wL)^2]_{t-1} = \frac{E[(\partial_wL)^2]_{t-1}}{1-\gamma^{\,\,t-1}}$ where $E[(\partial_{w}L)^2]_{t-1} = \gamma\, E[(\partial_{w}L)^2]_{t-2} + (1-\gamma)(\partial_{w^{\,t-1}}L)^2$.</li>
<li>$\hat{E}[\partial_wL]_{t-1}\frac{E[\partial_wL]_{t-1}}{1-\delta^{\,t-1}}$ where $E[\partial_wL]_{t-1} = \delta E[\partial_wL]_{t-2} + (1-\delta)\partial_{w^{\,t-1}}L$.</li>
<li>The boundary cases: $E[(\partial_{w}L)^2]_0 = 0$ and $E[\partial_wL]_0 = 0$.</li>
<li>Typical $\alpha = 0.001$, $\gamma = 0.999$, $\delta = 0.9$, $\epsilon = 10^{-7}$</li>
</ul>
<h2 id="Scribing-Note"><a href="#Scribing-Note" class="headerlink" title="Scribing Note"></a>Scribing Note</h2>

	<div class="row">
    <embed src="/myBlog/pdfs/aist4010_scribing2.pdf" width="100%" height="550" type="application/pdf">
	</div>



<!-- 
<img src="" width="500px" alt="" />
<font color="3A75EA">Blue</font>
<font color="F54747">Red</font>
<font color="880ED4">Purple</font>
<font color="00A300">Green</font>
-->
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://github.com/DonaldLamNL/myBlog">Donald Lam</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://github.com/DonaldLamNL/myBlog/2024/02/17/CUHK/AIST4010/2024-2-17-AIST4010-Optimization/">https://github.com/DonaldLamNL/myBlog/2024/02/17/CUHK/AIST4010/2024-2-17-AIST4010-Optimization/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/myBlog/tags/Deep-Learning/">Deep Learning</a></div><div class="post_share"><div class="social-share" data-image="https://p.ipic.vip/llqylk.jpg" data-sites="facebook,twitter"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/myBlog/2024/02/18/CUHK/AIST4010/2024-2-18-AIST4010-Hyper-paramater/"><img class="prev-cover" src="https://p.ipic.vip/llqylk.jpg" onerror="onerror=null;src='/myBlog/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Hyper-paramater</div></div></a></div><div class="next-post pull-right"><a href="/myBlog/2024/02/17/CUHK/ESTR4316/2024-1-25-ESTR4316-Kubernates/"><img class="next-cover" src="https://p.ipic.vip/hwdr0p.jpg" onerror="onerror=null;src='/myBlog/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Kubernetes Cluster Setup</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/myBlog/2023/06/26/CUHK/AIST4010/2023-6-26-CS231N-Image-Classification/" title="Image Classification"><img class="cover" src="https://p.ipic.vip/y8a6p2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-26</div><div class="title">Image Classification</div></div></a></div><div><a href="/myBlog/2024/01/28/CUHK/AIST4010/2024-1-28-AIST4010-Neural-Network%20copy/" title="Neural Network"><img class="cover" src="https://p.ipic.vip/llqylk.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-28</div><div class="title">Neural Network</div></div></a></div><div><a href="/myBlog/2024/01/30/CUHK/AIST4010/2024-1-30-AIST4010-Convolutional-Neural-Network/" title="Convolutional Neural Network"><img class="cover" src="https://p.ipic.vip/llqylk.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">Convolutional Neural Network</div></div></a></div><div><a href="/myBlog/2024/02/18/CUHK/AIST4010/2024-2-18-AIST4010-Hyper-paramater/" title="Hyper-paramater"><img class="cover" src="https://p.ipic.vip/llqylk.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-18</div><div class="title">Hyper-paramater</div></div></a></div><div><a href="/myBlog/2024/02/25/CUHK/AIST4010/2024-2-25-AIST4010-Text-Processing/" title="Text Processing"><img class="cover" src="https://p.ipic.vip/llqylk.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-25</div><div class="title">Text Processing</div></div></a></div><div><a href="/myBlog/2024/02/28/CUHK/AIST4010/2024-2-28-AIST4010-Recurrent-Neural-Networks/" title="Recurrent Neural Networks"><img class="cover" src="https://p.ipic.vip/llqylk.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-28</div><div class="title">Recurrent Neural Networks</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://p.ipic.vip/5dc8z2.jpeg" onerror="this.onerror=null;this.src='/myBlog/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Donald Lam</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/myBlog/archives/"><div class="headline">Articles</div><div class="length-num">71</div></a><a href="/myBlog/tags/"><div class="headline">Tags</div><div class="length-num">14</div></a><a href="/myBlog/categories/"><div class="headline">Categories</div><div class="length-num">14</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://donaldlamnl.github.io/myWeb/#/home"><i></i><span>My Personal Website</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/DonaldLamNL" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:manholam8@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Gradient-Descent"><span class="toc-number">1.</span> <span class="toc-text">Gradient Descent</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Batch-Gradient-Descent"><span class="toc-number">1.1.</span> <span class="toc-text">Batch Gradient Descent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Accumulate-Gradient"><span class="toc-number">1.2.</span> <span class="toc-text">Accumulate Gradient</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Momentum-Optimization"><span class="toc-number">2.</span> <span class="toc-text">Momentum Optimization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Momentum"><span class="toc-number">2.1.</span> <span class="toc-text">Momentum</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Nestorov%E2%80%99s-Momentum"><span class="toc-number">2.2.</span> <span class="toc-text">Nestorov’s Momentum</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scaling"><span class="toc-number">3.</span> <span class="toc-text">Scaling</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Inverse-mean-square-derivative"><span class="toc-number">3.1.</span> <span class="toc-text">Inverse mean square derivative</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Root-Mean-Squared-Propagation"><span class="toc-number">3.2.</span> <span class="toc-text">Root Mean Squared Propagation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adaptive-moment-estimation"><span class="toc-number">4.</span> <span class="toc-text">Adaptive moment estimation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RMS-Prop-Momentum"><span class="toc-number">4.1.</span> <span class="toc-text">RMS Prop + Momentum</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Expected-running-average"><span class="toc-number">4.2.</span> <span class="toc-text">Expected running average</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Running-average-correction"><span class="toc-number">4.3.</span> <span class="toc-text">Running average correction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adam-formula"><span class="toc-number">4.4.</span> <span class="toc-text">Adam formula</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scribing-Note"><span class="toc-number">5.</span> <span class="toc-text">Scribing Note</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/myBlog/2024/03/11/CUHK/AIST4010/2024-3-11-AIST4010-Attention/" title="Attention"><img src="https://p.ipic.vip/llqylk.jpg" onerror="this.onerror=null;this.src='/myBlog/img/404.jpg'" alt="Attention"/></a><div class="content"><a class="title" href="/myBlog/2024/03/11/CUHK/AIST4010/2024-3-11-AIST4010-Attention/" title="Attention">Attention</a><time datetime="2024-03-10T16:00:00.000Z" title="Created 2024-03-11 00:00:00">2024-03-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/myBlog/2024/02/28/CUHK/AIST4010/2024-2-28-AIST4010-Recurrent-Neural-Networks/" title="Recurrent Neural Networks"><img src="https://p.ipic.vip/llqylk.jpg" onerror="this.onerror=null;this.src='/myBlog/img/404.jpg'" alt="Recurrent Neural Networks"/></a><div class="content"><a class="title" href="/myBlog/2024/02/28/CUHK/AIST4010/2024-2-28-AIST4010-Recurrent-Neural-Networks/" title="Recurrent Neural Networks">Recurrent Neural Networks</a><time datetime="2024-02-27T16:00:00.000Z" title="Created 2024-02-28 00:00:00">2024-02-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/myBlog/2024/02/25/CUHK/AIST4010/2024-2-25-AIST4010-Text-Processing/" title="Text Processing"><img src="https://p.ipic.vip/llqylk.jpg" onerror="this.onerror=null;this.src='/myBlog/img/404.jpg'" alt="Text Processing"/></a><div class="content"><a class="title" href="/myBlog/2024/02/25/CUHK/AIST4010/2024-2-25-AIST4010-Text-Processing/" title="Text Processing">Text Processing</a><time datetime="2024-02-24T16:00:00.000Z" title="Created 2024-02-25 00:00:00">2024-02-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/myBlog/2024/02/18/CUHK/AIST4010/2024-2-18-AIST4010-Hyper-paramater/" title="Hyper-paramater"><img src="https://p.ipic.vip/llqylk.jpg" onerror="this.onerror=null;this.src='/myBlog/img/404.jpg'" alt="Hyper-paramater"/></a><div class="content"><a class="title" href="/myBlog/2024/02/18/CUHK/AIST4010/2024-2-18-AIST4010-Hyper-paramater/" title="Hyper-paramater">Hyper-paramater</a><time datetime="2024-02-17T16:00:00.000Z" title="Created 2024-02-18 00:00:00">2024-02-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/myBlog/2024/02/17/CUHK/AIST4010/2024-2-17-AIST4010-Optimization/" title="Optimization"><img src="https://p.ipic.vip/llqylk.jpg" onerror="this.onerror=null;this.src='/myBlog/img/404.jpg'" alt="Optimization"/></a><div class="content"><a class="title" href="/myBlog/2024/02/17/CUHK/AIST4010/2024-2-17-AIST4010-Optimization/" title="Optimization">Optimization</a><time datetime="2024-02-16T16:00:00.000Z" title="Created 2024-02-17 00:00:00">2024-02-17</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2024 By Donald Lam</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/myBlog/js/utils.js"></script><script src="/myBlog/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/myBlog/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>