<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Word Embedding | myBlog</title><meta name="keywords" content="Deep Learning"><meta name="author" content="Donald Lam,manholam8@gmail.com"><meta name="copyright" content="Donald Lam"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="Word EmbeddingOne-hot encoding Problems  High-dimension: Dimension of encoded data is equivalent to the number of different tokens within the dictionary Sparse:  Lack of semantic meaning: we want the">
<meta property="og:type" content="article">
<meta property="og:title" content="Word Embedding">
<meta property="og:url" content="https://github.com/DonaldLamNL/myBlog/2024/03/14/CUHK/AIST4010/2024-3-14-AIST4010-Word-Embedding/index.html">
<meta property="og:site_name" content="myBlog">
<meta property="og:description" content="Word EmbeddingOne-hot encoding Problems  High-dimension: Dimension of encoded data is equivalent to the number of different tokens within the dictionary Sparse:  Lack of semantic meaning: we want the">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://p.ipic.vip/llqylk.jpg">
<meta property="article:published_time" content="2024-03-13T16:00:00.000Z">
<meta property="article:modified_time" content="2024-03-13T16:00:00.000Z">
<meta property="article:author" content="Donald Lam">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://p.ipic.vip/llqylk.jpg"><link rel="shortcut icon" href="/myBlog/img/favicon.png"><link rel="canonical" href="https://github.com/DonaldLamNL/myBlog/2024/03/14/CUHK/AIST4010/2024-3-14-AIST4010-Word-Embedding/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/myBlog/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/myBlog/',
  algolia: undefined,
  localSearch: {"path":"/myBlog/search.xml","preload":true,"languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Word Embedding',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-03-14 00:00:00'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://p.ipic.vip/5dc8z2.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/myBlog/archives/"><div class="headline">Articles</div><div class="length-num">73</div></a><a href="/myBlog/tags/"><div class="headline">Tags</div><div class="length-num">14</div></a><a href="/myBlog/categories/"><div class="headline">Categories</div><div class="length-num">14</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/myBlog/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/myBlog/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/myBlog/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/myBlog/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://p.ipic.vip/llqylk.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/myBlog/">myBlog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/myBlog/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/myBlog/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/myBlog/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/myBlog/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Word Embedding</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-03-13T16:00:00.000Z" title="Created 2024-03-14 00:00:00">2024-03-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-03-13T16:00:00.000Z" title="Updated 2024-03-14 00:00:00">2024-03-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/myBlog/categories/AIST4010/">AIST4010</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Word Embedding"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h2><h3 id="One-hot-encoding"><a href="#One-hot-encoding" class="headerlink" title="One-hot encoding"></a>One-hot encoding</h3><p><img src="https://p.ipic.vip/77c6qt.png" width="400px" alt="" /></p>
<p><strong><font color="880ED4">Problems</font></strong></p>
<ol>
<li>High-dimension: Dimension of encoded data is equivalent to the number of different tokens within the dictionary</li>
<li>Sparse: </li>
<li>Lack of semantic meaning: we want the embedding of the token to represent something that we can understand. <font color="00A300">For example, King-Queen = Man-Woman</font>. Ideally, we hope the minus of the embedding of “king” and “queen” is equivalent to the minus of the embedding of “man” and “woman” (the different between “king” and “queen” is the same as the different between “man” and “woman”)<br><img src="https://p.ipic.vip/t7u82o.png" width="500px" alt="" /></li>
</ol>
<p><strong>Example:</strong><br><img src="https://p.ipic.vip/oemhii.png" width="600px" alt="" /><br>However, human labeling is <font color="880ED4">time-consuming</font> and <font color="880ED4">lack of generalization</font>, human labeling is very subjective </p>
<h2 id="Traditional-Word-Embedding"><a href="#Traditional-Word-Embedding" class="headerlink" title="Traditional Word Embedding"></a>Traditional Word Embedding</h2><h3 id="Word-Embedding-Model"><a href="#Word-Embedding-Model" class="headerlink" title="Word Embedding Model"></a>Word Embedding Model</h3><p>We want a model for mapping word to embedding that can compact representations for each word with no label (i.e., every token has semantic meaning and can link to some other tokens)<br><img src="https://p.ipic.vip/y7mciv.png" width="150px" alt="" /><br>We need to define the loss function and the model architecture</p>
<ul>
<li>Fully connected neural network</li>
<li>Able to deal with unlabel data</li>
</ul>
<p><strong>Loss Function:</strong></p>
<ul>
<li><p>With the nice embedding, words can indicate their neighbor words in the sentences, and vice versa (if the embedding word is meaningful, then with its embedding and the model, we can probably infer other words in the sentences)</p>
</li>
<li><p>Example: The man loves his son<br><img src="https://p.ipic.vip/siswn4.png" width="500px" alt="" /></p>
</li>
</ul>
<h3 id="Skip-Gram-Model"><a href="#Skip-Gram-Model" class="headerlink" title="Skip-Gram Model"></a>Skip-Gram Model</h3><p>Assume $u$ and $v$ are the embeddings from the neural network model</p>
<ul>
<li>The probability of getting the word outputs (<font color="3A75EA">surrounding words</font>) based on the prior:<script type="math/tex; mode=display">P(w_o | w_c) = \frac{\exp(u_o^Tv_c)}{\sum_{i \in V}\exp(u_i^Tv_c)}</script></li>
<li>For a sentence with $T$ words, the probability of having the exact sentence with window size $m$:<script type="math/tex; mode=display">P(S) = \prod_{t=1}^T \prod_{-m \leq j \leq m} P(w^{(t+j)} | w^{(t)})</script></li>
<li>Loss function:<script type="math/tex; mode=display">L = -\sum_{t=1}^T \prod_{-m \leq j \leq m} \log P(w^{(t+j)} | w^{(t)})</script></li>
</ul>
<p>Idea: it uses the center word to predict the surrounding words within the window<br><img src="https://p.ipic.vip/86ug21.png" width="300px" alt="" /></p>
<h3 id="Continuous-Bag-of-Words-Model"><a href="#Continuous-Bag-of-Words-Model" class="headerlink" title="Continuous Bag of Words Model"></a>Continuous Bag of Words Model</h3><p>Assume $u$ and $v$ are the embeddings from the neural network model</p>
<ul>
<li><p>The probability of getting the word output (<font color="3A75EA">center word</font>) based on the prior:</p>
<script type="math/tex; mode=display">P(w_c | w_{o_1}, \dots, w_{o_{2m}}) = \frac{\exp\big(\frac{1}{2m}u_c^T(v_{o_1} + \dots + v_{o_{2m}})\big)}{\sum_{i \in V} \exp\big(\frac{1}{2m}u_i^T(v_{o_1} + \dots + v_{o_{2m}})\big)}</script></li>
<li><p>For a sentence with $T$ words, the probability of having the exact sentence with window size $m$:</p>
<script type="math/tex; mode=display">P(S) = \prod_{t=1}^T P(w^{(t)} | w^{(t-m)}, \dots w^{(t-1)}, w^{(t+1)}, \dots, w^{(t+m)})</script></li>
<li><p>Loss function:</p>
<script type="math/tex; mode=display">L = -\sum_{t=1}^T \log P(w^{(t)} | w^{(t-m)}, \dots w^{(t-1)}, w^{(t+1)}, \dots, w^{(t+m)})</script></li>
</ul>
<p>Idea: it uses the surrounding words to predict the center word within the window<br><img src="https://p.ipic.vip/h83ibd.png" width="300px" alt="" /></p>
<h3 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h3><ol>
<li><strong><font color="880ED4">Can’t consider the</font> <font color="F54747">sentence context</font> <font color="880ED4">for each word</font></strong><br>After the embedding is generated, the embedding for each word will be fixed. Although it uses sentence information to learn the embedding for each word, the embedding for each word will be fixed when we are using the embedding, so we cannot consider the context when using the embedding of each word. Although we consider the sentence context during the training, those embedding will be fixed during the application</li>
<li><strong><font color="880ED4">Negative embedding</font></strong><br>Suppose we want to generate $w_o$ (the center word), then we need to calculate the distribution of $w_o$ over all the tokens within the dictionary ($i \in V$), which means we need to consider the entire dictionary in order to get the distribution of a token. Therefore, the computational cost is very expensive and inefficient</li>
<li><strong><font color="880ED4">Context-dependent word embedding</font></strong><br>A word (exact token) can have multiple meanings in different contexts, so the embedding should be different in different contexts<br><img src="https://p.ipic.vip/ibvon9.png" width="400px" alt="" /></li>
</ol>
<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p><strong>Idea:</strong> With the attention machanism, we can generate the context-dependent embedding or hidden state of the query token, so we can apply the attention machansim for the word embedding task.</p>
<p><strong>Focusing on the Encoder within the Transformer:</strong> Given a sentence, generate an embedding, which is very good to be the embedding task</p>
<p><strong>BERT: Bidirectional Encoder Representations from Transformers</strong>, which only depends on the transformer encoder</p>
<p><strong>Bidirectional:</strong> With bidirectional LSTM, we can consider the paths from the future. In the attention model, for each of the tokens within the sentence, we will consider the token’s interaction against all the other tokens within the sentence, which means not just the token before it will be considered, it also considers the tokens after it</p>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><ol>
<li><p><strong>Task 1: Classification Task</strong><br> Within a sentence, it will mask some tokens and then predict the <font color="3A75EA">masked token</font><br> <img src="https://p.ipic.vip/9osz9d.png" width="500px" alt="" /><br> GOAL: Use the multi-layer attention model to get the embedding for all the input tokens and to predict the token <code>MASK</code>, and stack a prediction layer to predict the missing token <code>MASK</code> (Can use softmax to select the most likely token). Then, compare the prediction with the ground truth sentence in the training layer and compute the loss to make a good propagation and train the model</p>
</li>
<li><p><strong>Task 2: Binary Prediction Task</strong> (with 2 pairs of sentences as input)<br> If we have a paragraph, not just a separate sentence, we may want to predict whether 2 sentences are next to each other<br> <img src="https://p.ipic.vip/8shol9.png" width="500px" alt="" /><br> GOAL: To predict whether 2 sentences are next to each other. To get read of handling the variant length of the sentences, we have a token <code>CLS</code> put at the beginning of all sentences and <code>SEP</code> to separate the sentences</p>
<ul>
<li><code>CLS</code> is a reversed token for the embedding to stack another layer to make the prediction</li>
<li><strong>Question:</strong> Why the <code>CLS</code> token can consider the information of both sentence 1 and sentence 2?<br>  Due to the attention model, in attention model, each token interacts with all the tokens within the input, which means the <code>CLS</code> token will interact with sentence 1, the <code>SEP</code> token and sentence 2, so the information from sentence 1 and sentence 2 will be aggregated during the attention mechanism, such that it has enough information to predict whether 2 sentences are next to each other</li>
</ul>
</li>
</ol>
<ul>
<li><strong>Loss Function:</strong> Linear combination of the above two tasks</li>
</ul>
<h3 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h3><ol>
<li><p>As the feature extractor</p>
</li>
<li><p>As the pre-trained model before fine-tunning<br> Directly stack another layer to the BERT model, with the last layer related to the downstream task (parameters within the last layer are initialized randomly but the parameters within the BERT model are pre-trained )<br> <img src="https://p.ipic.vip/jz0xyh.png" width="500px" alt="" /></p>
</li>
</ol>
<ul>
<li>Pre-training BERT requirement<ul>
<li>Parameters: 340 millions</li>
<li>Corpus: BooksCorpus: 800M words, English Wikipedia: 2,500M words</li>
</ul>
</li>
</ul>
<h2 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h2><p>With skip-gram or continuous bag of words methods, the word embedding of each word is the same in different sentences (<font color="3A75EA">true</font>)<br>Attention model is context-dependent. For different sentences, the outputted hidden state of the query can be different (<font color="3A75EA">true</font>)<br>Skip-gram or continuous bag of words methods are supervised learning methods (<font color="3A75EA">false: they are unsupervised learning methods</font>)<br>Trained with different corpus, the embedding of the same word can be slightly different (<font color="3A75EA">true</font>)</p>
<!-- 
<img src="" width="500px" alt="" />
<font color="3A75EA">Blue</font>
<font color="F54747">Red</font>
<font color="880ED4">Purple</font>
<font color="00A300">Green</font>
--></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://github.com/DonaldLamNL/myBlog">Donald Lam</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://github.com/DonaldLamNL/myBlog/2024/03/14/CUHK/AIST4010/2024-3-14-AIST4010-Word-Embedding/">https://github.com/DonaldLamNL/myBlog/2024/03/14/CUHK/AIST4010/2024-3-14-AIST4010-Word-Embedding/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/myBlog/tags/Deep-Learning/">Deep Learning</a></div><div class="post_share"><div class="social-share" data-image="https://p.ipic.vip/llqylk.jpg" data-sites="facebook,twitter"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/myBlog/2024/03/11/CUHK/AIST4010/2024-3-11-AIST4010-Attention/"><img class="next-cover" src="https://p.ipic.vip/llqylk.jpg" onerror="onerror=null;src='/myBlog/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Attention</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/myBlog/2023/06/26/CUHK/AIST4010/2023-6-26-CS231N-Image-Classification/" title="Image Classification"><img class="cover" src="https://p.ipic.vip/y8a6p2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-26</div><div class="title">Image Classification</div></div></a></div><div><a href="/myBlog/2024/01/27/CUHK/AIST4010/2024-1-27-AIST4010-Logistic-Regression/" title="Logistic Regression"><img class="cover" src="https://p.ipic.vip/llqylk.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-27</div><div class="title">Logistic Regression</div></div></a></div><div><a href="/myBlog/2024/01/30/CUHK/AIST4010/2024-1-30-AIST4010-Convolutional-Neural-Network/" title="Convolutional Neural Network"><img class="cover" src="https://p.ipic.vip/llqylk.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-30</div><div class="title">Convolutional Neural Network</div></div></a></div><div><a href="/myBlog/2024/01/28/CUHK/AIST4010/2024-1-28-AIST4010-Neural-Network%20copy/" title="Neural Network"><img class="cover" src="https://p.ipic.vip/llqylk.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-28</div><div class="title">Neural Network</div></div></a></div><div><a href="/myBlog/2024/01/29/CUHK/AIST4010/2024-1-29-AIST4010-Overfitting/" title="Overfitting"><img class="cover" src="https://p.ipic.vip/llqylk.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-29</div><div class="title">Overfitting</div></div></a></div><div><a href="/myBlog/2024/02/17/CUHK/AIST4010/2024-2-17-AIST4010-Optimization/" title="Optimization"><img class="cover" src="https://p.ipic.vip/llqylk.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-17</div><div class="title">Optimization</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://p.ipic.vip/5dc8z2.jpeg" onerror="this.onerror=null;this.src='/myBlog/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Donald Lam</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/myBlog/archives/"><div class="headline">Articles</div><div class="length-num">73</div></a><a href="/myBlog/tags/"><div class="headline">Tags</div><div class="length-num">14</div></a><a href="/myBlog/categories/"><div class="headline">Categories</div><div class="length-num">14</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://donaldlamnl.github.io/myWeb/#/home"><i></i><span>My Personal Website</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/DonaldLamNL" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:manholam8@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Word-Embedding"><span class="toc-number">1.</span> <span class="toc-text">Word Embedding</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#One-hot-encoding"><span class="toc-number">1.1.</span> <span class="toc-text">One-hot encoding</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Traditional-Word-Embedding"><span class="toc-number">2.</span> <span class="toc-text">Traditional Word Embedding</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Word-Embedding-Model"><span class="toc-number">2.1.</span> <span class="toc-text">Word Embedding Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Skip-Gram-Model"><span class="toc-number">2.2.</span> <span class="toc-text">Skip-Gram Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Continuous-Bag-of-Words-Model"><span class="toc-number">2.3.</span> <span class="toc-text">Continuous Bag of Words Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Limitations"><span class="toc-number">2.4.</span> <span class="toc-text">Limitations</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BERT"><span class="toc-number">3.</span> <span class="toc-text">BERT</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Training"><span class="toc-number">3.1.</span> <span class="toc-text">Training</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Applications"><span class="toc-number">3.2.</span> <span class="toc-text">Applications</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Questions"><span class="toc-number">4.</span> <span class="toc-text">Questions</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/myBlog/2024/03/14/CUHK/AIST4010/2024-3-14-AIST4010-Word-Embedding/" title="Word Embedding"><img src="https://p.ipic.vip/llqylk.jpg" onerror="this.onerror=null;this.src='/myBlog/img/404.jpg'" alt="Word Embedding"/></a><div class="content"><a class="title" href="/myBlog/2024/03/14/CUHK/AIST4010/2024-3-14-AIST4010-Word-Embedding/" title="Word Embedding">Word Embedding</a><time datetime="2024-03-13T16:00:00.000Z" title="Created 2024-03-14 00:00:00">2024-03-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/myBlog/2024/03/11/CUHK/AIST4010/2024-3-11-AIST4010-Attention/" title="Attention"><img src="https://p.ipic.vip/llqylk.jpg" onerror="this.onerror=null;this.src='/myBlog/img/404.jpg'" alt="Attention"/></a><div class="content"><a class="title" href="/myBlog/2024/03/11/CUHK/AIST4010/2024-3-11-AIST4010-Attention/" title="Attention">Attention</a><time datetime="2024-03-10T16:00:00.000Z" title="Created 2024-03-11 00:00:00">2024-03-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/myBlog/2024/03/01/CUHK/AIST4010/2024-3-1-AIST4010-RNN-Model-Architectures/" title="RNN++ (Model Architectures)"><img src="https://p.ipic.vip/llqylk.jpg" onerror="this.onerror=null;this.src='/myBlog/img/404.jpg'" alt="RNN++ (Model Architectures)"/></a><div class="content"><a class="title" href="/myBlog/2024/03/01/CUHK/AIST4010/2024-3-1-AIST4010-RNN-Model-Architectures/" title="RNN++ (Model Architectures)">RNN++ (Model Architectures)</a><time datetime="2024-02-29T16:00:00.000Z" title="Created 2024-03-01 00:00:00">2024-03-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/myBlog/2024/02/28/CUHK/AIST4010/2024-2-28-AIST4010-Recurrent-Neural-Networks/" title="Recurrent Neural Networks"><img src="https://p.ipic.vip/llqylk.jpg" onerror="this.onerror=null;this.src='/myBlog/img/404.jpg'" alt="Recurrent Neural Networks"/></a><div class="content"><a class="title" href="/myBlog/2024/02/28/CUHK/AIST4010/2024-2-28-AIST4010-Recurrent-Neural-Networks/" title="Recurrent Neural Networks">Recurrent Neural Networks</a><time datetime="2024-02-27T16:00:00.000Z" title="Created 2024-02-28 00:00:00">2024-02-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/myBlog/2024/02/25/CUHK/AIST4010/2024-2-25-AIST4010-Text-Processing/" title="Text Processing"><img src="https://p.ipic.vip/llqylk.jpg" onerror="this.onerror=null;this.src='/myBlog/img/404.jpg'" alt="Text Processing"/></a><div class="content"><a class="title" href="/myBlog/2024/02/25/CUHK/AIST4010/2024-2-25-AIST4010-Text-Processing/" title="Text Processing">Text Processing</a><time datetime="2024-02-24T16:00:00.000Z" title="Created 2024-02-25 00:00:00">2024-02-25</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2024 By Donald Lam</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/myBlog/js/utils.js"></script><script src="/myBlog/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/myBlog/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>